{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Revis√£o PLN"
      ],
      "metadata": {
        "id": "v0q3NziOSenf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conte√∫do:\n",
        "\n",
        "*   **Tratamento de Texto:** C√≥digo feito em aula, mostrando limpeza do texto, tokeniza√ß√£o e cria√ß√£o de bag of words\n",
        "*   **T√©cnicas de Processamento de Texto:** Algoritmos independentes mostrando as etapas do processamento de texto, incluindo:\n",
        "        1.   Pr√©-processamento\n",
        "        2.   Remo√ß√£o de Stopwords\n",
        "        3.   Tokeniza√ß√£o\n",
        "        4.   One-Hot Encoding\n",
        "        5.   Vectorizer\n",
        "        6.   Stemming\n",
        "        7.   Lemmatization\n",
        "*   **Algoritmos de Treinamento:** Modelos de algoritmos de treinamento\n",
        "        1.   Rede Neural [Keras]\n",
        "        2.   LSTM\n",
        "*   **Exerc√≠cios:** Exerc√≠cios 1 a 4 (tarefas relacionadas com processamento de texto). Cada exerc√≠cio est√° dividido numa se√ß√£o\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iRzdfGz2SiBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________________________________\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fgHOz88wULXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tratamento de Texto"
      ],
      "metadata": {
        "id": "6RZcuWWYR3uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"rato roeu a roupa do rei de Roma,\n",
        "O rato roeu a roupa do rei da R√∫ssia,\n",
        "O rato roeu a roupa do RodovaIho...\n",
        "O rato a roer ro√≠a\n",
        "E a rosa Rita Ramalho do rato a roer se ria.\n",
        "O rato roeu a roupa do rei de roma\n",
        "a rainha com raiva roeu o resto.\"\"\"\n",
        "\n",
        "# Pr√©-processamento\n",
        "corpus_lower = corpus.lower()\n",
        "trans_table = str.maketrans('.,\\n', '   ')\n",
        "corpus_limpo = corpus_lower.translate(trans_table)\n",
        "\n",
        "# Criando as tokens a partir do Texto\n",
        "tokens = corpus_limpo.split(\" \")\n",
        "\n",
        "# Removendo os stop words\n",
        "stop_words = ['a', 'o', 'da', 'do', 'e', 'que', 'com', 'se', 'de']\n",
        "\n",
        "tokens_limpos = []\n",
        "for token in tokens:\n",
        "    if token != '' and token not in stop_words:\n",
        "        tokens_limpos.append( token )\n",
        "print(\"Tokens: \", tokens_limpos, \"\\n\")\n",
        "\n",
        "# Criando o Bag Of Words com contagem de palavras\n",
        "vocabulario = list(set(tokens_limpos))\n",
        "print(\"Vocabulario: \", vocabulario, \"\\n\")\n",
        "\n",
        "bag_of_words = {}\n",
        "for token in tokens_limpos:\n",
        "    if token in bag_of_words:\n",
        "        bag_of_words[token] = bag_of_words[token] + 1\n",
        "    else:\n",
        "        bag_of_words[token] = 1\n",
        "print(\"Bag of Words (com contagem): \", bag_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE6B4GR4R7zq",
        "outputId": "d34cbf19-d6f4-4d66-b2cb-46ec2518821a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['rato', 'roeu', 'roupa', 'rei', 'roma', 'rato', 'roeu', 'roupa', 'rei', 'r√∫ssia', 'rato', 'roeu', 'roupa', 'rodovaiho', 'rato', 'roer', 'ro√≠a', 'rosa', 'rita', 'ramalho', 'rato', 'roer', 'ria', 'rato', 'roeu', 'roupa', 'rei', 'roma', 'rainha', 'raiva', 'roeu', 'resto'] \n",
            "\n",
            "Vocabulario:  ['rato', 'r√∫ssia', 'resto', 'rei', 'ro√≠a', 'roupa', 'rainha', 'rosa', 'raiva', 'rodovaiho', 'ramalho', 'roeu', 'roer', 'ria', 'rita', 'roma'] \n",
            "\n",
            "Bag of Words (com contagem):  {'rato': 6, 'roeu': 5, 'roupa': 4, 'rei': 3, 'roma': 2, 'r√∫ssia': 1, 'rodovaiho': 1, 'roer': 2, 'ro√≠a': 1, 'rosa': 1, 'rita': 1, 'ramalho': 1, 'ria': 1, 'rainha': 1, 'raiva': 1, 'resto': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##T√©cnicas de Processamento de Texto"
      ],
      "metadata": {
        "id": "roStQVCk10Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pr√©-processamento de Texto\n"
      ],
      "metadata": {
        "id": "Ee8RM7JH14tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realiza o pr√©-processamento de texto, incluindo:\n",
        "    <br> - Convers√£o para min√∫sculas\n",
        "    <br> - Remo√ß√£o de acentua√ß√£o\n",
        "    <br> - Remo√ß√£o de pontua√ß√£o\n",
        "    <br> - Normaliza√ß√£o de espa√ßos"
      ],
      "metadata": {
        "id": "Jr7NQeCL5LmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Converte para min√∫sculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove acentua√ß√£o\n",
        "    text = unidecode(text)\n",
        "\n",
        "    # Remove pontua√ß√£o\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove espa√ßos extras\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Exemplo de uso\n",
        "texto_exemplo = \"Ol√°, Mundo! Eu tenho 42 ma√ß√£s. üòä\"\n",
        "print(preprocess_text(texto_exemplo))"
      ],
      "metadata": {
        "id": "QwVBfyq0IeHF",
        "outputId": "fa1435d0-c375-456b-d8bd-dfb68d1e8a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ola mundo eu tenho 42 macas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remo√ß√£o de Stopwords\n"
      ],
      "metadata": {
        "id": "Z-ws3EPZ90iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords s√£o palavras que n√£o agregam sentido, como artigos, preposi√ß√µes, conjun√ß√µes, pronomes e etc."
      ],
      "metadata": {
        "id": "J_gFv4qi-JZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Certifique-se de baixar as stopwords antes: nltk.download('stopwords')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remover_stopwords(texto):\n",
        "    # Obtemos a lista de stopwords para o idioma desejado (portugu√™s neste caso).\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "    # Dividimos o texto em uma lista de palavras.\n",
        "    palavras = texto.split()\n",
        "\n",
        "    # Retornamos apenas as palavras que n√£o est√£o na lista de stopwords.\n",
        "    return \" \".join([palavra for palavra in palavras if palavra not in stop_words])\n",
        "\n",
        "# Exemplo de uso\n",
        "texto_exemplo = \"Este √© um exemplo de texto com algumas palavras irrelevantes.\"\n",
        "print(remover_stopwords(texto_exemplo))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCsH0sBE-JyJ",
        "outputId": "accba160-7d9c-4ec3-e044-7126dc3949c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Este exemplo texto algumas palavras irrelevantes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokeniza√ß√£o [Tokenizer]"
      ],
      "metadata": {
        "id": "AEe0_isj_UvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniza√ß√£o √© o processo de dividir um texto em palavras individuais (tokens). Neste exemplo, a fun√ß√£o word_tokenize do NLTK  transforma o texto em uma lista de palavras, facilitando o processamento subsequente."
      ],
      "metadata": {
        "id": "8zlkTwNV_jGX"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the necessary data for tokenization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenizar_texto(texto):\n",
        "    return word_tokenize(texto)\n",
        "\n",
        "# Exemplo de uso\n",
        "texto_exemplo = \"Este √© um exemplo de tokeniza√ß√£o.\"\n",
        "print(tokenizar_texto(texto_exemplo))\n",
        "\n",
        "# Itera√ß√£o sobre os tokens\n",
        "tokens = tokenizar_texto(texto_exemplo)\n",
        "for i, token in enumerate(tokens, start=1):\n",
        "    print(f\"Token {i}: {token}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Sk48XlAoYk",
        "outputId": "8957d5ee-cb4f-4b04-cf57-9cbb703fc96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Este', '√©', 'um', 'exemplo', 'de', 'tokeniza√ß√£o', '.']\n",
            "Token 1: Este\n",
            "Token 2: √©\n",
            "Token 3: um\n",
            "Token 4: exemplo\n",
            "Token 5: de\n",
            "Token 6: tokeniza√ß√£o\n",
            "Token 7: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###One-Hot Encoding"
      ],
      "metadata": {
        "id": "85dQBJNkCKAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforma palavras √∫nicas em vetores bin√°rios. Cada palavra √© representada por um vetor onde somente uma posi√ß√£o √© \"1\", indicando a presen√ßa daquela palavra. Isso √© √∫til para algoritmos que n√£o conseguem processar texto diretamente."
      ],
      "metadata": {
        "id": "As0HFLmPCaUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def one_hot_encoding(lista_de_palavras):\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    # Convertemos a lista de palavras em um formato adequado para o encoder.\n",
        "    palavras_transformadas = [[palavra] for palavra in lista_de_palavras]\n",
        "    return encoder.fit_transform(palavras_transformadas)\n",
        "\n",
        "# Exemplo de uso\n",
        "texto_exemplo = \"cachorro gato gato cachorro peixe\"\n",
        "tokens = texto_exemplo.split()\n",
        "one_hot = one_hot_encoding(tokens)\n",
        "print(one_hot)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz6JWy_oCPlh",
        "outputId": "262221cf-0c3b-41ee-cc5a-eb9b942abce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Vetoriza√ß√£o [Vectorizer - Bag of Words]"
      ],
      "metadata": {
        "id": "ZwcfEw2CDIzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words cria uma matriz onde cada linha representa um texto e cada coluna, uma palavra √∫nica do vocabul√°rio. Os valores indicam o n√∫mero de vezes que uma palavra aparece no texto."
      ],
      "metadata": {
        "id": "ZFtKWAXoDO-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def bag_of_words(textos):\n",
        "    vectorizer = CountVectorizer()\n",
        "    # Ajusta o vetorizador ao texto e transforma.\n",
        "    matriz = vectorizer.fit_transform(textos)\n",
        "    return matriz.toarray(), vectorizer.get_feature_names_out()\n",
        "\n",
        "# Exemplo de uso\n",
        "textos_exemplo = [\"cachorro gato peixe\", \"gato peixe\", \"cachorro cachorro gato\"]\n",
        "matriz, palavras = bag_of_words(textos_exemplo)\n",
        "print(\"Matriz:\\n\", matriz)\n",
        "print(\"Palavras:\\n\", palavras)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BrtYqcTDPkA",
        "outputId": "9a120d9f-e70f-498b-b8f3-5ebe5bdddde5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz:\n",
            " [[1 1 1]\n",
            " [0 1 1]\n",
            " [2 1 0]]\n",
            "Palavras:\n",
            " ['cachorro' 'gato' 'peixe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normatiza√ß√£o - Stemming\n"
      ],
      "metadata": {
        "id": "B_FrOdZoEM_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O stemming reduz as palavras √† sua forma b√°sica (radical). √â √∫til para considerar diferentes formas gramaticais como equivalentes (ex.: \"correr\", \"corremos\")."
      ],
      "metadata": {
        "id": "77ZBwp9AEQ7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download('rslp')\n",
        "\n",
        "def aplicar_stemming(texto):\n",
        "    stemmer = RSLPStemmer()  # Stemmer espec√≠fico para o portugu√™s.\n",
        "    palavras = texto.split()  # Dividimos o texto em palavras.\n",
        "    # Aplicamos o stemming em cada palavra.\n",
        "    return \" \".join([stemmer.stem(palavra) for palavra in palavras])\n",
        "\n",
        "# Exemplo de uso\n",
        "texto_exemplo = \"corremos correr√° correndo corredores\"\n",
        "print(aplicar_stemming(texto_exemplo))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVpUVYvgERSO",
        "outputId": "ac805563-c812-46e5-e57d-4a52549f55da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corr corr corr corr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normatiza√ß√£o - Lemmatization\n"
      ],
      "metadata": {
        "id": "D_i9It8BEmGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lematiza√ß√£o √© semelhante ao stemming, mas mais precisa. Enquanto o stemming corta palavras para obter o radical, a lematiza√ß√£o considera o contexto para obter a forma base correta."
      ],
      "metadata": {
        "id": "gJk3PQsIEtYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Certifique-se de instalar o modelo: python -m spacy download pt_core_news_sm\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "def aplicar_lemmatizacao(texto):\n",
        "    doc = nlp(texto)  # Processamos o texto com Spacy.\n",
        "    # Retornamos os lemas de cada token.\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "# Exemplo de uso\n",
        "texto_exemplo = \"Os meninos correram para pegar o trem.\"\n",
        "print(aplicar_lemmatizacao(texto_exemplo))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7yZ_LIWEttF",
        "outputId": "8b5f3959-bb26-48bc-a871-aeaef9f3ee7e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o menino correr para pegar ele tr .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Algoritmos de Treinamento"
      ],
      "metadata": {
        "id": "tNdKPetlMORM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rede Neural [Keras] - Classifica√ß√£o de Texto"
      ],
      "metadata": {
        "id": "6-6BNtIFMSqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifica textos em diferentes categorias (por exemplo, sentimento positivo ou negativo)."
      ],
      "metadata": {
        "id": "cVSEQPgzWrx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Flatten\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Exemplo de dados de texto\n",
        "textos = [\"Eu amo este produto\", \"Muito ruim, n√£o gostei\", \"Excelente, recomendo\", \"Horr√≠vel, n√£o vale a pena\", \"Produto maravilhoso\"]\n",
        "labels = [1, 0, 1, 0, 1]  # 1 = positivo, 0 = negativo\n",
        "\n",
        "# Tokeniza√ß√£o do texto\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(textos)\n",
        "X = tokenizer.texts_to_sequences(textos)\n",
        "\n",
        "# Padding para que todas as sequ√™ncias tenham o mesmo tamanho\n",
        "X_pad = pad_sequences(X, padding='post')\n",
        "\n",
        "# Criar o modelo\n",
        "modelo = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=X_pad.shape[1]),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sa√≠da bin√°ria (positivo ou negativo)\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "modelo.fit(X_pad, np.array(labels), epochs=10)\n",
        "\n",
        "# Avaliar o modelo\n",
        "print(modelo.evaluate(X_pad, np.array(labels)))"
      ],
      "metadata": {
        "id": "pNbOrmLhWzcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM - Classifica√ß√£o de Texto"
      ],
      "metadata": {
        "id": "gcSkrNe7W2-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "√ìtimo para processar sequ√™ncias de texto, como an√°lises de sentimentos ou predi√ß√£o de t√≥picos, permitindo que o modelo lembre de informa√ß√µes anteriores da sequ√™ncia."
      ],
      "metadata": {
        "id": "VKZNhcDfXApJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Exemplo de dados de texto\n",
        "textos = [\"Eu adoro programa√ß√£o\", \"Detesto erros de c√≥digo\", \"Muito f√°cil de aprender\", \"Erros s√£o frustrantes\", \"Aprender programa√ß√£o √© divertido\"]\n",
        "labels = [1, 0, 1, 0, 1]  # 1 = positivo, 0 = negativo\n",
        "\n",
        "# Tokeniza√ß√£o do texto\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(textos)\n",
        "X = tokenizer.texts_to_sequences(textos)\n",
        "\n",
        "# Padding para que todas as sequ√™ncias tenham o mesmo tamanho\n",
        "X_pad = pad_sequences(X, padding='post')\n",
        "\n",
        "# Criar o modelo LSTM\n",
        "modelo = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=X_pad.shape[1]),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Sa√≠da bin√°ria (positivo ou negativo)\n",
        "])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "modelo.fit(X_pad, np.array(labels), epochs=10)\n",
        "\n",
        "# Avaliar o modelo\n",
        "print(modelo.evaluate(X_pad, np.array(labels)))"
      ],
      "metadata": {
        "id": "HEdPwpdcXFQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exerc√≠cios"
      ],
      "metadata": {
        "id": "lYi5NXZq9gey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exerc√≠cio 1"
      ],
      "metadata": {
        "id": "zOSnUKCu9oHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com base no texto, fa√ßa um c√≥digo para extrair o texto que est√° entre\n",
        "o primeiro (.) e a primeira (,). Utilize a fun√ß√£o find para localizar\n",
        "as posi√ß√µes do ponto e da virgula no texto e, em seguida, fa√ßa um la√ßo\n",
        "for para gerar um texto novo com os caracteres entre o ponto e a virgula"
      ],
      "metadata": {
        "id": "dOeJg-T9GH-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Python √© uma linguagem de programa√ß√£o poderosa e vers√°til.\\\n",
        "        √â amplamente utilizada em desenvolvimento web, ci√™ncia de\\\n",
        "        dados, intelig√™ncia artificial e muito mais.\"\n",
        "\n",
        "# Encontrar o ponto\n",
        "posicao_ponto = texto.find(\".\")\n",
        "\n",
        "# Definir a primeira v√≠rgula ap√≥s o ponto\n",
        "# obs: posicao_ponto serve como argumento para estabelcer o in√≠cio da busca\n",
        "posicao_virgula = texto.find(\",\", posicao_ponto)\n",
        "\n",
        "# Necess√°rio inicializar a vari√°vel antes do la√ßo como string vazia\n",
        "novo_texto = \"\"\n",
        "\n",
        "# La√ßo for para gerar o novo texto\n",
        "for i in range(posicao_ponto + 1, posicao_virgula):\n",
        "    novo_texto += texto[i]\n",
        "\n",
        "# Remover caracteres em branco no in√≠cio (lstrip, √† esquerda) e final (rstrip, √† direita)\n",
        "novo_texto = novo_texto.lstrip().rstrip()\n",
        "\n",
        "print(f\"Texto extra√≠do: '{novo_texto}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myBcXG0GGYlK",
        "outputId": "13e69f1d-fd39-459a-8bbb-85d754d0bc2d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto extra√≠do: '√â amplamente utilizada em desenvolvimento web'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exerc√≠cio 2"
      ],
      "metadata": {
        "id": "yQ5KJjGBGeC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Crie uma lista de dicion√°rios para armazenar coment√°rios e emo√ß√µes conforme a tabela <br>\n",
        "2) Fa√ßa uma rotina para totalizar a quantidade de coment√°rios negativos e coment√°rios positivos <br>\n",
        "3) Calcule e mostre na tela a propor√ß√£o de cada sentimento em rela√ß√£o ao total de coment√°rios <br>\n",
        "4) Fa√ßa uma rotina para mostrar apenas os coment√°rios positivos <br>\n",
        "5) Adicione uma chave em cada dicion√°rio chamado sentimento_valor que conter√° 0 se o sentimento for negativo ou 1 se o sentimento for positivo"
      ],
      "metadata": {
        "id": "OCN_xPHZGf9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar vari√°veis\n",
        "total_comentarios = 0\n",
        "comentarios_positivos = 0\n",
        "comentarios_negativos = 0\n",
        "\n",
        "# Criar lista de dicion√°rios\n",
        "dicionarios = [\n",
        "    { \"autor\": \"Jo√£o\", \"comentario\": \"Estou t√£o feliz hoje!\", \"sentimento_valor\": 1 },\n",
        "    { \"autor\": \"Maria\", \"comentario\": \"Este filme √© t√£o triste.\", \"sentimento_valor\": 0 },\n",
        "    { \"autor\": \"Carlos\", \"comentario\": \"Que dia chuvoso entediante...\", \"sentimento_valor\": 0 },\n",
        "    { \"autor\": \"Ana\", \"comentario\": \"Adorei a nova m√∫sica da banda!\", \"sentimento_valor\": 1 },\n",
        "    { \"autor\": \"Roberto\", \"comentario\": \"Eureka, consegui resolver este problema\", \"sentimento_valor\": 1 }\n",
        "]\n",
        "\n",
        "# Totalizar coment√°rios\n",
        "for comentario in dicionarios:\n",
        "    total_comentarios += 1\n",
        "print(f\"H√° {total_comentarios} coment√°rios ao todo.\\n\")\n",
        "\n",
        "# Exibir coment√°rios positivos\n",
        "print(\"Coment√°rios Positivos:\")\n",
        "\n",
        "for comentario in dicionarios:\n",
        "    if comentario[\"sentimento_valor\"] == 1:\n",
        "        print(f\"{comentario['autor']}: {comentario['comentario']}\")\n",
        "        # Contabilizar coment√°rios positivos\n",
        "        comentarios_positivos += 1\n",
        "# Definir propor√ß√£o de coment√°rios positivos\n",
        "proporcao_positivos = (comentarios_positivos / total_comentarios) *100\n",
        "print(f\"H√° {comentarios_positivos} coment√°rio(s) positivo(s), representado {proporcao_positivos}% do total de coment√°rios.\\n\")\n",
        "\n",
        "# Exibir coment√°rios negativos\n",
        "print(\"Coment√°rios Negativos:\")\n",
        "\n",
        "for comentario in dicionarios:\n",
        "    if comentario[\"sentimento_valor\"] == 0:\n",
        "        print(f\"{comentario['autor']}: {comentario['comentario']}\")\n",
        "        # Contabilizar coment√°rios negativos\n",
        "        comentarios_negativos += 1\n",
        "# Definir propor√ß√£o de coment√°rios negativos\n",
        "proporcao_negativos = (comentarios_negativos / total_comentarios) *100\n",
        "print(f\"H√° {comentarios_negativos} coment√°rio(s) negativo(s), representado {proporcao_negativos}% do total de coment√°rios.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eWG5mpyGncH",
        "outputId": "b24d5d04-a2e5-4a29-d1f9-e52c6cd254bf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H√° 5 coment√°rios ao todo.\n",
            "\n",
            "Coment√°rios Positivos:\n",
            "Jo√£o: Estou t√£o feliz hoje!\n",
            "Ana: Adorei a nova m√∫sica da banda!\n",
            "Roberto: Eureka, consegui resolver este problema\n",
            "H√° 3 coment√°rio(s) positivo(s), representado 60.0% do total de coment√°rios.\n",
            "\n",
            "Coment√°rios Negativos:\n",
            "Maria: Este filme √© t√£o triste.\n",
            "Carlos: Que dia chuvoso entediante...\n",
            "H√° 2 coment√°rio(s) negativo(s), representado 40.0% do total de coment√°rios.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exerc√≠cio 3"
      ],
      "metadata": {
        "id": "lC6bEo23Gr1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contexto: Voc√™ foi contratado por uma empresa de marketing digital para analisar coment√°rios de clientes em uma rede social. Para come√ßar a an√°lise, √© necess√°rio limpar os dados e deix√°-los em um formato adequado para a aplica√ß√£o de algoritmos de NLP.\n",
        "\n",
        "Tarefa: Crie uma fun√ß√£o que...\n",
        "<br> 1) Converta todas as palavras para min√∫sculas.\n",
        "<br> 2) Elimine caracteres especiais (pontua√ß√£o, emojis, etc.).\n",
        "<br> 3) Retorne uma lista com as palavras pr√©-processadas\n",
        "\n",
        "Exemplo de par√¢metro: \"Adorei o produto! Super recomendo üòç\"\n",
        "Exemplo de retorno: [\"adorei\", \"o\", \"produto\", \"super\", \"recomendo\"]"
      ],
      "metadata": {
        "id": "vGqrF2fqGtTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Criar fun√ß√£o\n",
        "def preprocessar_comentario(comentario):\n",
        "    # Converter as palavras em min√∫sculas\n",
        "    comentario = comentario.lower()\n",
        "\n",
        "    # Eliminar caracteres especiais (pontua√ß√£o, emojis, etc.), utilizando regex\n",
        "    comentario = re.sub(r'[^\\w\\s]', '', comentario)  # Remove pontua√ß√£o\n",
        "    comentario = re.sub(r'\\d+', '', comentario)  # Remove n√∫meros (se necess√°rio)\n",
        "\n",
        "    # Separar o coment√°rio em palavras\n",
        "    palavras = comentario.split()\n",
        "\n",
        "    # Retornar lista de palavras pr√©-processadas\n",
        "    return palavras\n",
        "\n",
        "# Uso com coment√°rio passado como par√¢metro\n",
        "exemplo = \"Adorei o produto! Super recomendo üòç\"\n",
        "resultado = preprocessar_comentario(exemplo)\n",
        "print(f\"Exemplo: Adorei o produto! Super recomendo üòç  -> Resultado: {resultado}\\n\")\n",
        "\n",
        "# Uso com entrada do usu√°rio\n",
        "comentario_usuario = input(\"Digite um coment√°rio: \")\n",
        "resultado = preprocessar_comentario(comentario_usuario)\n",
        "print(f\"Coment√°rio pr√©-processado: {resultado}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqqeIoPnG37I",
        "outputId": "75e3004e-07f5-4cfd-cebc-d345c6c9053b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemplo: Adorei o produto! Super recomendo üòç  -> Resultado: ['adorei', 'o', 'produto', 'super', 'recomendo']\n",
            "\n",
            "Digite um coment√°rio: Ol√°, como vai voc√™, hein?!\n",
            "Coment√°rio pr√©-processado: ['ol√°', 'como', 'vai', 'voc√™', 'hein']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exerc√≠cio 4"
      ],
      "metadata": {
        "id": "Az27gAqTHGOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contexto: Voc√™ trabalha em uma startup que oferece suporte via mensagens de texto. As mensagens dos clientes s√£o escritas de maneira informal, com abrevia√ß√µes e erros de digita√ß√£o. Sua tarefa √© normalizar esses textos para facilitar a an√°lise.\n",
        "\n",
        "Tarefa:\n",
        "<br> 1) Identifique e corrija abrevia√ß√µes e erros comuns (por exemplo, \"vc\" ‚Üí \"voc√™\", \"eh\" ‚Üí \"√©\").\n",
        "<br> 2) Remova g√≠rias e express√µes que n√£o adicionam valor sem√¢ntico.\n",
        "<br> 3) Retorne o texto normalizado.\n",
        "\n",
        "Exemplo de entrada: \"Vc eh mt bom, recomendo mto!!!\"\n",
        "Exemplo de sa√≠da: \"voc√™ √© muito bom recomendo muito\"\n",
        "\n",
        "Dica: Construa um dicion√°rio para de g√≠rias de maneira que a chave seja a g√≠ria e o valor a palavra formal."
      ],
      "metadata": {
        "id": "uS89ErXOHHuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Dicion√°rio de g√≠rias\n",
        "girias = { \"mto\": \"muito\", \"mt\": \"muito\", \"vc\": \"voc√™\", \"eh\": \"√©\" }\n",
        "\"\"\"\n",
        "    Como h√° duas chaves com valores iguais, √© necess√°rio manter\n",
        "    a chave mais longa primeiro para n√£o afetar a sa√≠da e\n",
        "    duplicar caracteres no momento da substitui√ß√£o\n",
        "\"\"\"\n",
        "\n",
        "comentario = \"Vc eh mt bom, recomendo mto!!!\"\n",
        "\n",
        "# Transformar o texto em min√∫sculo\n",
        "comentario = comentario.lower()\n",
        "\n",
        "# Eliminar g√≠rias e express√µes que n√£o adicionam valor sem√¢ntico\n",
        "comentario = re.sub(r'[^\\w\\s]', '', comentario)  # Remove pontua√ß√£o\n",
        "comentario = re.sub(r'\\d+', '', comentario)  # Remove n√∫meros (se necess√°rio)\n",
        "\n",
        "# Substituir as chaves (abrevia√ß√µes) do dicion√°rio pelos seus valores (corre√ß√µes)\n",
        "for abreviacao, correcao in girias.items():\n",
        "    comentario = comentario.replace(abreviacao, correcao)\n",
        "\n",
        "# Remover espa√ßos extras\n",
        "comentario = ' '.join(comentario.split())\n",
        "\n",
        "print(f\"Coment√°rio corrigido: '{comentario}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUWhHHQkHS6c",
        "outputId": "f7f173be-5274-4996-c16c-723373f51e38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coment√°rio corrigido: 'voc√™ √© muito bom recomendo muito'\n"
          ]
        }
      ]
    }
  ]
}